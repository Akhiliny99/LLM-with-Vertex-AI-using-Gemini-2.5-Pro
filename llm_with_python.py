# -*- coding: utf-8 -*-
"""LLM with Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FVQPZfqKXQt3dKnqN8Zc4Pvx-yQVqqah
"""

!pip install google-cloud-aiplatform

from vertexai.preview.language_models import TextGenerationModel
import vertexai

import os


os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = ""


print("Service account JSON key file is set.")

!pip install -q google-cloud-aiplatform

from vertexai import init
from vertexai.generative_models import GenerativeModel


init(project="llm-with-python", location="us-central1")


model = GenerativeModel("publishers/google/models/gemini-2.5-pro-preview-05-06")


response = model.generate_content("Hello world!")

print("Response from LLM:")
print(response.text)

response = model.generate_content("Tell me a funny joke.")
print("Response from LLM:")
print(response.text)

prompt = "Write a short poem about data engineering."
response = model.generate_content(prompt)
print("Response from LLM:")
print(response.text)

from vertexai.generative_models import GenerativeModel, ChatSession

model = GenerativeModel("publishers/google/models/gemini-2.5-pro-preview-05-06")

chat = model.start_chat()


response1 = chat.send_message("Hi there! How are you?")
print("Response from LLM:")
print(response1.text)


response2 = chat.send_message("Tell me a fun fact about Python.")
print("Response from LLM:")
print(response2.text)

response = model.generate_content(prompt)
print("Raw response object:", response)
print("Candidates:", response.candidates)


for c in response.candidates:
    print("Candidate:", c)

from vertexai.generative_models import GenerativeModel

model = GenerativeModel("publishers/google/models/gemini-2.5-pro-preview-05-06")


prompts = [
    "Why is Python widely used in data engineering?",
    "What are the main components of a modern data pipeline?",
    "How do batch processing and stream processing differ in data engineering?",
    "What is the role of Apache Spark in large-scale data processing?",
    "Can you explain the architecture of a cloud-based data warehouse like BigQuery or Snowflake?",
    "What tools are commonly used for ETL (Extract, Transform, Load) workflows?",
    "How do data engineers ensure data quality and integrity in pipelines?",
    "What is the difference between a data lake and a data warehouse?",
    "What are the benefits of using Apache Airflow for pipeline orchestration?",
    "How does schema evolution work in big data platforms like Hive or Parquet?",
    "What are the key challenges in real-time data processing?",
    "How do you handle slowly changing dimensions (SCDs) in a data warehouse?",
    "Why is data partitioning important in big data systems?",
    "Whatâ€™s the future demand and evolving role of data engineers in the AI era?"
]


for prompt in prompts:
    response = model.generate_content(prompt)
    print(f"Prompt: {prompt}")
    print(f"LLM Response: {response.text}\n")
